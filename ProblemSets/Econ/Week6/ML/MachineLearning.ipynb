{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estulate the probability that a given wine comes from a given _cultivar_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report\n",
    "from pylab import rcParams\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import dataset\n",
    "\n",
    "wine = pd.read_csv('strongdrink.txt', sep=\",\")\n",
    "#wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A: Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nb: categorical data because 3 categories\n",
    "\n",
    "The logistic regression model is a binary dependent variable classifier that constraints its predicted values to be strictly between 0 and 1. This is done by using a sigmoid function :\n",
    "\n",
    "$f(x) = g(x) / (1 + g(x))$\n",
    "\n",
    "In the case of the Logistic function : \n",
    "$g(B) = e^{X*B} = e^{B0 + B1*x1 + B2*x2 + B3*x3 .... + Bp*xp}$\n",
    "\n",
    "How to estimate the best theta = {B0, B1, B2, ..., Bp) ?\n",
    "\n",
    "#### Nonlinear Least Squares Estimation\n",
    "\n",
    "$\\epsilon_i = y_i - z_i$\n",
    "$\\theta$ st $min_\\theta \\sum(e_i)^2$\n",
    "\n",
    "$z_i = Pr(y_i = 1 | X_i, \\theta)$\n",
    "\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "MLH because it has the most robust statistical properties.\n",
    "\n",
    "$\\theta_{mle} = \\theta : max_\\theta ln(L(y_i, Xi|\\theta)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated coefficiens and intercepts for j=1  -20.292802602916396 [ 1.25230292 -0.2049683   1.45813599  0.40918121]\n",
      "Estimated coefficiens and intercepts for j=2  20.920605961450406 [-1.05123233 -0.44005868  0.55536801 -1.71913401]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      1.00      0.83        12\n",
      "           2       1.00      0.75      0.86        24\n",
      "           3       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.84        44\n",
      "   macro avg       0.83      0.88      0.84        44\n",
      "weighted avg       0.88      0.84      0.84        44\n",
      "\n",
      "Computed MSE on average is  0.1590909090909091 \n",
      " Computed MSE for y1 is  0.29411764705882354 \n",
      " Computed MSE for y2 is  0.0 \n",
      " Computed MSE for y3 is  0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "### Divide the data between X and Y\n",
    "X = wine[[\"alco\", \"malic\", \"tot_phen\", \"color_int\"]].values\n",
    "y = wine[\"cultivar\"].values\n",
    "\n",
    "### Add a constant to the X\n",
    "#num_obs = X.shape[0]\n",
    "#const_vec = np.ones(num_obs).reshape((num_obs, 1))\n",
    "#XplusConst = np.hstack((const_vec, X))\n",
    "\n",
    "### Divide the data between test and train\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size = 0.25,\n",
    "       random_state=10)\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "\n",
    "\n",
    "### Run the MLHE of out multiple logistic regression classifier model.\n",
    "LogReg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "#LogReg = LogisticRegression()\n",
    "\n",
    "LogReg.fit(X_train, y_train)\n",
    "y_pred = LogReg.predict(X_test)\n",
    "\n",
    "print(\"Estimated coefficiens and intercepts for j=1 \", LogReg.intercept_[0], LogReg.coef_[0,:])\n",
    "print(\"Estimated coefficiens and intercepts for j=2 \", LogReg.intercept_[1], LogReg.coef_[1,:])\n",
    "\n",
    "### Report the error rates (1 - precision)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "### Manually MSE\n",
    "MSE_raw = ((y_test - y_pred) ** 2)\n",
    "for i in range(MSE_raw.shape[0]):\n",
    "    MSE_raw[i] = min(MSE_raw[i], 1)\n",
    "MSE_y = MSE_raw.mean()\n",
    "MSE_y1 = MSE_raw[y_pred==1].mean()\n",
    "MSE_y2 = MSE_raw[y_pred==2].mean()\n",
    "MSE_y3 = MSE_raw[y_pred==3].mean()\n",
    "\n",
    "print(\"Computed MSE on average is \", MSE_y,\n",
    "     \"\\n Computed MSE for y1 is \", MSE_y1,\n",
    "     \"\\n Computed MSE for y2 is \", MSE_y2,\n",
    "     \"\\n Computed MSE for y3 is \", MSE_y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categories that are best predicted by the model are 2 > 1 > 3. This corresponds to the categories with the most observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "71\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "print(wine[wine[\"cultivar\"]==1].shape[0])\n",
    "print(wine[wine[\"cultivar\"]==2].shape[0])\n",
    "print(wine[wine[\"cultivar\"]==3].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Leave-one-out cross validation (LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loo as a leave-one-out object, then split it into N different partitions\n",
    "\n",
    "N_loo = X.shape[0]\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "MSE_vec = np.zeros(N_loo)\n",
    "MSE_y1 = np.zeros(N_loo)\n",
    "MSE_y2 = np.zeros(N_loo)\n",
    "MSE_y3 = np.zeros(N_loo)\n",
    "\n",
    "y_test_total = np.zeros(N_loo)\n",
    "y_pred_total= np.zeros(N_loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test estimate MSE loocv= 0.07954545454545454 , test estimate MSE standard err= 0.2705882022679695\n"
     ]
    }
   ],
   "source": [
    "# Loop\n",
    "for train_index, test_index in loo.split(X):\n",
    "    # Split x and y between test and train\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #y_test_total[train_index] = y_test\n",
    "    \n",
    "    # Run the logit on the training set MULTINOMIAL\n",
    "    LogReg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "    LogReg.fit(X_train, y_train)\n",
    "    # Predict the train y\n",
    "    y_pred = LogReg.predict(X_test)\n",
    "    MSE_raw = (y_pred - y_test) ** 2\n",
    "    y_pred_total[test_index] = y_pred\n",
    "    for i in range(MSE_raw.shape[0]):\n",
    "        MSE_raw[i] = min(1, MSE_raw[i])\n",
    "    # Get the error for each category\n",
    "    MSE_vec[test_index] = MSE_raw\n",
    "    #MSE_y1[test_index] = MSE_raw[y_pred==1]\n",
    "    #MSE_y2[test_index] = MSE_raw[y_pred==2]\n",
    "    #MSE_y3[test_index] = MSE_raw[y_pred==3]\n",
    "\n",
    "    #print('MSE for test set', test_index, ' is', MSE_vec[test_index])\n",
    "\n",
    "MSE_loo = MSE_vec.mean()\n",
    "MSE_loo_std = MSE_vec.std()\n",
    "print('test estimate MSE loocv=', MSE_loo,\n",
    "      ', test estimate MSE standard err=', MSE_loo_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV estimate for the test MSE as the average MSE is  0.07954545454545454\n",
      "LOOCV estimate for the test MSE for y=1 is  0.09836065573770492\n",
      "LOOCV estimate for the test MSE for y=2 is  0.08571428571428572\n",
      "LOOCV estimate for the test MSE for y=3 is  0.044444444444444446\n"
     ]
    }
   ],
   "source": [
    "MSE_loo_1 = MSE_vec[y_pred_total==1].mean()\n",
    "MSE_loo_2 = MSE_vec[y_pred_total==2].mean()\n",
    "MSE_loo_3 = MSE_vec[y_pred_total==3].mean()\n",
    "\n",
    "\n",
    "print(\"LOOCV estimate for the test MSE as the average MSE is \", MSE_vec.mean())\n",
    "\n",
    "print(\"LOOCV estimate for the test MSE for y=1 is \", MSE_loo_1)\n",
    "\n",
    "print(\"LOOCV estimate for the test MSE for y=2 is \", MSE_loo_2)\n",
    "\n",
    "print(\"LOOCV estimate for the test MSE for y=3 is \", MSE_loo_3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test estimate MSE k-fold= 0.09090909090909091 \n",
      " test estimate MSE standard err= 0.04821182598999188 \n",
      " test estimate MSE k-fold for y1 = 0.11856617647058823 \n",
      " test estimate MSE k-fold for y2 = 0.09375 \n",
      " test estimate MSE k-fold for y3 = 0.05555555555555555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k=4\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=10)\n",
    "kf.get_n_splits(X)\n",
    "y_test_total = np.zeros(k)\n",
    "MSE_vec_kf = np.zeros(k)\n",
    "MSE_test_y1 = np.zeros(k)\n",
    "MSE_test_y2 = np.zeros(k)\n",
    "MSE_test_y3 = np.zeros(k)\n",
    "\n",
    "\n",
    "k_ind = int(0)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    # print('k index=', k_ind)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    LogReg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "    LogReg.fit(X_train, y_train)\n",
    "    y_pred = LogReg.predict(X_test)\n",
    "    #print(y_pred)\n",
    "    error_raw = (y_test - y_pred) ** 2\n",
    "    for i in range(error_raw.shape[0]):\n",
    "        error_raw[i] = min(error_raw[i], 1)\n",
    "    error_raw_y1 = error_raw[y_pred==1]\n",
    "    error_raw_y2 = error_raw[y_pred==2]\n",
    "    error_raw_y3 = error_raw[y_pred==3]\n",
    "\n",
    "    MSE_vec_kf[k_ind] = error_raw.mean()\n",
    "    MSE_test_y1[k_ind] = error_raw_y1.mean()\n",
    "    MSE_test_y2[k_ind] = error_raw_y2.mean()\n",
    "    MSE_test_y3[k_ind] = error_raw_y3.mean()\n",
    "\n",
    "# print('MSE for test set', k_ind, ' is', MSE_vec_kf[k_ind])\n",
    "    k_ind += 1\n",
    "\n",
    "MSE_kf = MSE_vec_kf.mean()\n",
    "MSE_kf_std = MSE_vec_kf.std()\n",
    "MSE_kf1 = MSE_test_y1.mean()\n",
    "MSE_kf2 = MSE_test_y2.mean()\n",
    "MSE_kf3 = MSE_test_y3.mean()\n",
    "\n",
    "print('test estimate MSE k-fold=', MSE_kf,\n",
    "      '\\n test estimate MSE standard err=', MSE_kf_std,\n",
    "     '\\n test estimate MSE k-fold for y1 =', MSE_kf1,\n",
    "     '\\n test estimate MSE k-fold for y2 =', MSE_kf2,\n",
    "     '\\n test estimate MSE k-fold for y3 =', MSE_kf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import dataset\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "biden = pd.read_csv('biden.csv', sep=\",\")\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1807 entries, 0 to 1806\n",
      "Data columns (total 6 columns):\n",
      "biden     1807 non-null int64\n",
      "female    1807 non-null int64\n",
      "age       1807 non-null int64\n",
      "educ      1807 non-null int64\n",
      "dem       1807 non-null int64\n",
      "rep       1807 non-null int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 98.8 KB\n",
      "None\n",
      "[90 70 60 ... 70 60 15]\n",
      "[[ 0 19  1  0 12]\n",
      " [ 1 51  1  0 14]\n",
      " [ 0 27  0  0 14]\n",
      " ...\n",
      " [ 1 60  1  0 12]\n",
      " [ 1 91  1  0  8]\n",
      " [ 1 34  0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "# Split between training and test\n",
    "### Divide the data between test and train\n",
    "biden = biden.dropna()\n",
    "X = biden['biden'].values\n",
    "y = biden[[\"female\", \"age\", \"dem\", \"rep\", \"educ\"]].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.3,\n",
    "    random_state=25)\n",
    "\n",
    "\n",
    "print(biden.info())\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=4,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Use recursive binary splitting to fit a decision tree to the training data\n",
    "# biden = response variable\n",
    "# other variables = predictores\n",
    "\n",
    "max_depth  = 3\n",
    "min_samples_leaf=5\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .4,\n",
    "                                                    random_state=25)\n",
    "hit_tree2 = DecisionTreeRegressor(max_depth=3, min_samples_leaf=4)\n",
    "hit_tree2.fit(X_train.reshape(-1,1), y_train)\n",
    "\n",
    "\n",
    "### Need to split between training and test sets!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"776pt\" height=\"613pt\"\n",
       " viewBox=\"0.00 0.00 776.00 613.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 609)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-609 772,-609 772,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#e99356\" stroke=\"#000000\" d=\"M429.5,-605C429.5,-605 337.5,-605 337.5,-605 331.5,-605 325.5,-599 325.5,-593 325.5,-593 325.5,-489 325.5,-489 325.5,-483 331.5,-477 337.5,-477 337.5,-477 429.5,-477 429.5,-477 435.5,-477 441.5,-483 441.5,-489 441.5,-489 441.5,-593 441.5,-593 441.5,-599 435.5,-605 429.5,-605\"/>\n",
       "<text text-anchor=\"middle\" x=\"383.5\" y=\"-589.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">X[0] &lt;= 82.5</text>\n",
       "<text text-anchor=\"middle\" x=\"383.5\" y=\"-574.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 59.132</text>\n",
       "<text text-anchor=\"middle\" x=\"383.5\" y=\"-559.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1084</text>\n",
       "<text text-anchor=\"middle\" x=\"383.5\" y=\"-544.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.55]</text>\n",
       "<text text-anchor=\"middle\" x=\"383.5\" y=\"-529.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[47.465]</text>\n",
       "<text text-anchor=\"middle\" x=\"383.5\" y=\"-514.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.437]</text>\n",
       "<text text-anchor=\"middle\" x=\"383.5\" y=\"-499.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.202]</text>\n",
       "<text text-anchor=\"middle\" x=\"383.5\" y=\"-484.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[13.335]]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e78946\" stroke=\"#000000\" d=\"M360.5,-441C360.5,-441 276.5,-441 276.5,-441 270.5,-441 264.5,-435 264.5,-429 264.5,-429 264.5,-325 264.5,-325 264.5,-319 270.5,-313 276.5,-313 276.5,-313 360.5,-313 360.5,-313 366.5,-313 372.5,-319 372.5,-325 372.5,-325 372.5,-429 372.5,-429 372.5,-435 366.5,-441 360.5,-441\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-425.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">X[0] &lt;= 45.0</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-410.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 55.829</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-395.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 771</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-380.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.52]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-365.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[46.332]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-350.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.31]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-335.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.274]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-320.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[13.418]]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M357.9968,-476.6536C354.6013,-468.0864 351.0989,-459.2494 347.6474,-450.5411\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"350.868,-449.1677 343.9296,-441.1609 344.3605,-451.747 350.868,-449.1677\"/>\n",
       "<text text-anchor=\"middle\" x=\"334.0091\" y=\"-460.4083\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"#eda673\" stroke=\"#000000\" d=\"M493,-441C493,-441 404,-441 404,-441 398,-441 392,-435 392,-429 392,-429 392,-325 392,-325 392,-319 398,-313 404,-313 404,-313 493,-313 493,-313 499,-313 505,-319 505,-325 505,-325 505,-429 505,-429 505,-435 499,-441 493,-441\"/>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-425.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">X[0] &lt;= 89.0</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-410.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 65.027</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-395.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 313</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-380.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.623]</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-365.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[50.256]</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-350.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.751]</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-335.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.026]</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-320.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[13.131]]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M409.0032,-476.6536C412.3987,-468.0864 415.9011,-459.2494 419.3526,-450.5411\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"422.6395,-451.747 423.0704,-441.1609 416.132,-449.1677 422.6395,-451.747\"/>\n",
       "<text text-anchor=\"middle\" x=\"432.9909\" y=\"-460.4083\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#e78c4b\" stroke=\"#000000\" d=\"M232,-277C232,-277 143,-277 143,-277 137,-277 131,-271 131,-265 131,-265 131,-161 131,-161 131,-155 137,-149 143,-149 143,-149 232,-149 232,-149 238,-149 244,-155 244,-161 244,-161 244,-265 244,-265 244,-271 238,-277 232,-277\"/>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-261.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">X[0] &lt;= 4.0</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-246.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 56.846</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-231.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 187</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-216.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.428]</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-201.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[50.422]</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-186.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.086]</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.572]</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[13.898]]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M267.1013,-312.6536C259.8941,-303.6307 252.4477,-294.3086 245.1341,-285.1526\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"247.7263,-282.7898 238.7505,-277.1609 242.257,-287.1587 247.7263,-282.7898\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#e5833d\" stroke=\"#000000\" d=\"M360.5,-277C360.5,-277 276.5,-277 276.5,-277 270.5,-277 264.5,-271 264.5,-265 264.5,-265 264.5,-161 264.5,-161 264.5,-155 270.5,-149 276.5,-149 276.5,-149 360.5,-149 360.5,-149 366.5,-149 372.5,-155 372.5,-161 372.5,-161 372.5,-265 372.5,-265 372.5,-271 366.5,-277 360.5,-277\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-261.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">X[0] &lt;= 52.5</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-246.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 54.057</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-231.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 584</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-216.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.55]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-201.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[45.022]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-186.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.382]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.178]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[13.264]]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M318.5,-312.6536C318.5,-304.3599 318.5,-295.8132 318.5,-287.3752\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"322.0001,-287.1608 318.5,-277.1609 315.0001,-287.1609 322.0001,-287.1608\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#ea985d\" stroke=\"#000000\" d=\"M101,-113C101,-113 12,-113 12,-113 6,-113 0,-107 0,-101 0,-101 0,-12 0,-12 0,-6 6,0 12,0 12,0 101,0 101,0 107,0 113,-6 113,-12 113,-12 113,-101 113,-101 113,-107 107,-113 101,-113\"/>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-97.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 60.494</text>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-82.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 31</text>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-67.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.258]</text>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[54.645]</text>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.032]</text>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.677]</text>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[13.935]]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.8393,-148.8939C126.111,-139.6612 118.1552,-130.1568 110.4218,-120.918\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"113.0905,-118.6533 103.9879,-113.2317 107.7228,-123.1465 113.0905,-118.6533\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#e68743\" stroke=\"#000000\" d=\"M232,-113C232,-113 143,-113 143,-113 137,-113 131,-107 131,-101 131,-101 131,-12 131,-12 131,-6 137,0 143,0 143,0 232,0 232,0 238,0 244,-6 244,-12 244,-12 244,-101 244,-101 244,-107 238,-113 232,-113\"/>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-97.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 55.269</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-82.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 156</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-67.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.462]</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[49.583]</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.096]</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.551]</text>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[13.891]]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M187.5,-148.8939C187.5,-140.4921 187.5,-131.8654 187.5,-123.4185\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"191.0001,-123.2317 187.5,-113.2317 184.0001,-123.2318 191.0001,-123.2317\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#e58139\" stroke=\"#000000\" d=\"M363,-113C363,-113 274,-113 274,-113 268,-113 262,-107 262,-101 262,-101 262,-12 262,-12 262,-6 268,0 274,0 274,0 363,0 363,0 369,0 375,-6 375,-12 375,-12 375,-101 375,-101 375,-107 369,-113 363,-113\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-97.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 53.311</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-82.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 218</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-67.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.555]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[44.009]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.252]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.275]</text>\n",
       "<text text-anchor=\"middle\" x=\"318.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[13.041]]</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M318.5,-148.8939C318.5,-140.4921 318.5,-131.8654 318.5,-123.4185\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"322.0001,-123.2317 318.5,-113.2317 315.0001,-123.2318 322.0001,-123.2317\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"#e6843e\" stroke=\"#000000\" d=\"M494,-113C494,-113 405,-113 405,-113 399,-113 393,-107 393,-101 393,-101 393,-12 393,-12 393,-6 399,0 405,0 405,0 494,0 494,0 500,0 506,-6 506,-12 506,-12 506,-101 506,-101 506,-107 500,-113 494,-113\"/>\n",
       "<text text-anchor=\"middle\" x=\"449.5\" y=\"-97.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 54.292</text>\n",
       "<text text-anchor=\"middle\" x=\"449.5\" y=\"-82.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 366</text>\n",
       "<text text-anchor=\"middle\" x=\"449.5\" y=\"-67.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.546]</text>\n",
       "<text text-anchor=\"middle\" x=\"449.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[45.626]</text>\n",
       "<text text-anchor=\"middle\" x=\"449.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.459]</text>\n",
       "<text text-anchor=\"middle\" x=\"449.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.12]</text>\n",
       "<text text-anchor=\"middle\" x=\"449.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[13.396]]</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M372.1607,-148.8939C379.889,-139.6612 387.8448,-130.1568 395.5782,-120.918\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"398.2772,-123.1465 402.0121,-113.2317 392.9095,-118.6533 398.2772,-123.1465\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"#eba069\" stroke=\"#000000\" d=\"M493,-269.5C493,-269.5 404,-269.5 404,-269.5 398,-269.5 392,-263.5 392,-257.5 392,-257.5 392,-168.5 392,-168.5 392,-162.5 398,-156.5 404,-156.5 404,-156.5 493,-156.5 493,-156.5 499,-156.5 505,-162.5 505,-168.5 505,-168.5 505,-257.5 505,-257.5 505,-263.5 499,-269.5 493,-269.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-254.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 63.035</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-239.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 212</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-224.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.599]</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-209.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[49.58]</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-194.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.731]</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-179.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.028]</text>\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[13.335]]</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M448.5,-312.6536C448.5,-301.9089 448.5,-290.7397 448.5,-279.9322\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"452.0001,-279.7068 448.5,-269.7068 445.0001,-279.7069 452.0001,-279.7068\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<path fill=\"#efb185\" stroke=\"#000000\" d=\"M625,-277C625,-277 536,-277 536,-277 530,-277 524,-271 524,-265 524,-265 524,-161 524,-161 524,-155 530,-149 536,-149 536,-149 625,-149 625,-149 631,-149 637,-155 637,-161 637,-161 637,-265 637,-265 637,-271 631,-277 625,-277\"/>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-261.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">X[0] &lt;= 92.5</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-246.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 68.559</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-231.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 101</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-216.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.673]</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-201.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[51.673]</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-186.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.792]</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.02]</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[12.703]]</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>8&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M500.291,-312.6536C507.5533,-303.6307 515.0565,-294.3086 522.426,-285.1526\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"525.3147,-287.1456 528.8583,-277.1609 519.8616,-282.7565 525.3147,-287.1456\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"#ffffff\" stroke=\"#000000\" d=\"M625,-113C625,-113 536,-113 536,-113 530,-113 524,-107 524,-101 524,-101 524,-12 524,-12 524,-6 530,0 536,0 536,0 625,0 625,0 631,0 637,-6 637,-12 637,-12 637,-101 637,-101 637,-107 631,-113 625,-113\"/>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-97.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 93.057</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-82.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 9</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-67.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.667]</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[52.889]</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.556]</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.111]</text>\n",
       "<text text-anchor=\"middle\" x=\"580.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[13.778]]</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M580.5,-148.8939C580.5,-140.4921 580.5,-131.8654 580.5,-123.4185\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"584.0001,-123.2317 580.5,-113.2317 577.0001,-123.2318 584.0001,-123.2317\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"#edaa79\" stroke=\"#000000\" d=\"M756,-113C756,-113 667,-113 667,-113 661,-113 655,-107 655,-101 655,-101 655,-12 655,-12 655,-6 661,0 667,0 667,0 756,0 756,0 762,0 768,-6 768,-12 768,-12 768,-101 768,-101 768,-107 762,-113 756,-113\"/>\n",
       "<text text-anchor=\"middle\" x=\"711.5\" y=\"-97.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 66.104</text>\n",
       "<text text-anchor=\"middle\" x=\"711.5\" y=\"-82.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 92</text>\n",
       "<text text-anchor=\"middle\" x=\"711.5\" y=\"-67.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [[0.674]</text>\n",
       "<text text-anchor=\"middle\" x=\"711.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[51.554]</text>\n",
       "<text text-anchor=\"middle\" x=\"711.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.815]</text>\n",
       "<text text-anchor=\"middle\" x=\"711.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[0.011]</text>\n",
       "<text text-anchor=\"middle\" x=\"711.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">[12.598]]</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M634.1607,-148.8939C641.889,-139.6612 649.8448,-130.1568 657.5782,-120.918\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"660.2772,-123.1465 664.0121,-113.2317 654.9095,-118.6533 660.2772,-123.1465\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1c205d1550>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_tree_viz = export_graphviz(\n",
    "    hit_tree2,\n",
    "    out_file=None,\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "graph = graphviz.Source(hit_tree_viz)\n",
    "graph.render('hit_tree_viz')\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE= 57.10089592812896\n"
     ]
    }
   ],
   "source": [
    "y_pred = hit_tree2.predict(X_test.reshape(-1,1))\n",
    "MSE1 = mean_squared_error(y_test, y_pred)\n",
    "print('MSE=', MSE1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Randomizes search CV with optimally tuned hyperparameters from part a's decision tree.\n",
    "\n",
    "Takes random permutations of the hyperparameters and chooses the permutation that has the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandBestEstimator1= DecisionTreeRegressor(criterion='mse', max_depth=10, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=16,\n",
      "                      min_samples_split=9, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "RandBestParams1= {'max_depth': 10, 'min_samples_leaf': 16, 'min_samples_split': 9}\n",
      "RandBestScore1= 59.930284075444774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist1 = {'max_depth': [3, 10],\n",
    "             'min_samples_split': sp_randint(2, 20),\n",
    "             'min_samples_leaf': sp_randint(2, 20)}\n",
    "hittree3 = DecisionTreeRegressor()\n",
    "\n",
    "# Run randomized hyperparameter search\n",
    "random_search1 = \\\n",
    "    RandomizedSearchCV(hittree3, param_distributions=param_dist1,\n",
    "                       n_iter=500, n_jobs=-1, cv=5, random_state=25,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "\n",
    "random_search1.fit(X.reshape(-1,1), y)\n",
    "print('RandBestEstimator1=', random_search1.best_estimator_)\n",
    "print('RandBestParams1=', random_search1.best_params_)\n",
    "print('RandBestScore1=', -random_search1.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Tune the parameters of a RF regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB score= 0.026379990485624775\n",
      "MSE= 58.126276189440375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/base.py:420: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "### Simple Random Forest\n",
    "\n",
    "B4 =100\n",
    "m4 = 1\n",
    "hit_tree4 = RandomForestRegressor(n_estimators=B4, max_features=m4, bootstrap=True,\n",
    "                                  n_jobs=-1, oob_score=True, random_state=25)\n",
    "\n",
    "\n",
    "hit_tree4.fit(X.reshape(-1,1), y)\n",
    "\n",
    "print('OOB score=', hit_tree4.score(X.reshape(-1,1), y))\n",
    "y_pred4 = hit_tree4.oob_prediction_\n",
    "MSE4 = mean_squared_error(y, y_pred4)\n",
    "print('MSE=', MSE4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:737: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. rnd MSE= 58.06362147518628 , Min. rnd B= 79\n"
     ]
    }
   ],
   "source": [
    "### Random Forest + Bootstrapping\n",
    "\n",
    "max_samples = 100\n",
    "MSE_vec4 = np.zeros(max_samples)\n",
    "for ind in range(1, max_samples + 1):\n",
    "    hit_tree4_loop = RandomForestRegressor(n_estimators=ind, \n",
    "                                           max_features='sqrt',\n",
    "                                           bootstrap=True, \n",
    "                                           oob_score=True,\n",
    "                                           random_state=15)\n",
    "    hit_tree4_loop.fit(X.reshape(-1,1), y)\n",
    "    y_pred4_loop = hit_tree4_loop.oob_prediction_\n",
    "    MSE_vec4[ind - 1] = mean_squared_error(y, y_pred4_loop)\n",
    "    # print('MSE=', MSE_vec[ind - 1])\n",
    "\n",
    "print('Min. rnd MSE=', MSE_vec4.min(), ', Min. rnd B=', np.argwhere(MSE_vec4 == MSE_vec4.min())[0, 0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandBestEstimator1= RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=3,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=14, min_samples_split=9,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "RandBestParams1= {'max_depth': 3, 'min_samples_leaf': 14, 'min_samples_split': 9}\n",
      "RandBestScore1= 59.616201303139434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "### Simple Random Forest + Tunning parameters\n",
    "\n",
    "param_dist2 = {'n_estimators': [10, 200],\n",
    "               \"max_depth\": [3, 10],\n",
    "             \"min_samples_split\": sp_randint(2, 20),\n",
    "             \"min_samples_leaf\": sp_randint(2, 20),\n",
    "             \"max_features\": sp_randint(1, 5)}\n",
    "\n",
    "\n",
    "hit_tree4_loop = RandomForestRegressor()\n",
    "\n",
    "\n",
    "# Run randomized hyperparameter search\n",
    "random_search2 = \\\n",
    "    RandomizedSearchCV(hit_tree4_loop, param_distributions=param_dist1,\n",
    "                       n_iter=500, n_jobs=-1, cv=5, random_state=25,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "\n",
    "random_search2.fit(X.reshape(-1,1), y)\n",
    "print('RandBestEstimator1=', random_search2.best_estimator_)\n",
    "print('RandBestParams1=', random_search2.best_params_)\n",
    "print('RandBestScore1=', -random_search2.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classifier \"horse\" race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv('auto.csv', sep=\",\", header=0)\n",
    "auto['horsepower'] = auto['horsepower'].apply(pd.to_numeric, downcast='float', errors='coerce')\n",
    "auto.dropna(inplace=True)\n",
    "\n",
    "auto[\"mpg_high\"] = (auto[\"mpg\"] >= auto[\"mpg\"].median())*1.\n",
    "auto[\"orgn1\"] = (auto[\"origin\"]==1)*1.\n",
    "auto[\"orgn2\"] = (auto[\"origin\"]==2)*1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the factors that make miles per gallon high or low\n",
    "auto[\"mpg_high\"] = (auto[\"mpg\"] >= auto[\"mpg\"].median())*1.\n",
    "y_auto = auto[\"mpg_high\"].values\n",
    "X = auto[[\"cylinders\", \"displacement\", \"horsepower\", \n",
    "          \"weight\", \"acceleration\", \"year\", \"orgn1\", \"orgn2\"]].values\n",
    "\n",
    "# Add a constant\n",
    "num_obs = X.shape[0]\n",
    "const_vec = np.ones(num_obs).reshape((num_obs, 1))\n",
    "XplusConst_auto = np.hstack((const_vec, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Fit a logistic model of mpg_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test estimate MSE k-fold= 0.09948979591836735 \n",
      " test estimate MSE standard err= 0.027356646160111246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k=4\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=25)\n",
    "kf.get_n_splits(XplusConst_auto)\n",
    "y_test_total = np.zeros(k)\n",
    "MSE_vec_kf = np.zeros(k)\n",
    "\n",
    "\n",
    "k_ind = int(0)\n",
    "for train_index, test_index in kf.split(XplusConst_auto):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    # print('k index=', k_ind)\n",
    "    X_train, X_test = XplusConst_auto[train_index], XplusConst_auto[test_index]\n",
    "    y_train, y_test = y_auto[train_index], y_auto[test_index]\n",
    "    LogReg = LogisticRegression()\n",
    "    LogReg.fit(X_train, y_train)\n",
    "    y_pred = LogReg.predict(X_test)\n",
    "    #print(y_pred)\n",
    "    error_raw = (y_test - y_pred) ** 2\n",
    "\n",
    "    MSE_vec_kf[k_ind] = error_raw.mean()\n",
    "\n",
    "# print('MSE for test set', k_ind, ' is', MSE_vec_kf[k_ind])\n",
    "    k_ind += 1\n",
    "\n",
    "MSE_kf = MSE_vec_kf.mean()\n",
    "MSE_kf_std = MSE_vec_kf.std()\n",
    "\n",
    "print('test estimate MSE k-fold=', MSE_kf,\n",
    "      '\\n test estimate MSE standard err=', MSE_kf_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandBestEstimator1= RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
      "                      max_features=1, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=10, min_samples_split=18,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "RandBestParams1= {'max_depth': 8, 'max_features': 1, 'min_samples_leaf': 10, 'min_samples_split': 18, 'n_estimators': 10}\n",
      "RandBestScore1= 0.08958089905000387\n"
     ]
    }
   ],
   "source": [
    "### Simple Random Forest + Tunning parameters\n",
    "\n",
    "param_dist2 = {'n_estimators': [10, 200],\n",
    "               \"max_depth\": [3, 8],\n",
    "               \"min_samples_split\": sp_randint(2, 20),\n",
    "               \"min_samples_leaf\": sp_randint(2, 20),\n",
    "               \"max_features\": sp_randint(1, 8)}\n",
    "\n",
    "\n",
    "hit_tree4_loop = RandomForestRegressor()\n",
    "\n",
    "\n",
    "# Run randomized hyperparameter search\n",
    "random_search2 = \\\n",
    "    RandomizedSearchCV(hit_tree4_loop, param_distributions=param_dist2,\n",
    "                       n_iter=100, n_jobs=-1, cv=4, random_state=25,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "\n",
    "random_search2.fit(XplusConst_auto, y_auto)\n",
    "print('RandBestEstimator1=', random_search2.best_estimator_)\n",
    "print('RandBestParams1=', random_search2.best_params_)\n",
    "print('RandBestScore1=', -random_search2.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. support vector machines model with a Gaussian radial basis function kernel. \n",
    "\n",
    "Use sklearn.svm.SVC to fit the model.\n",
    "Optimally tune the hyperparameters in the support vector machines classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB score= 0.8979591836734694\n",
      "MSE= 0.10204081632653061\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform as sp_uniform \n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "### Divide the data between test and train\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(XplusConst_auto, y_auto, test_size = 0.25,\n",
    "       random_state=10)\n",
    "\n",
    "auto_svc = svm.SVC(kernel='linear', C=2)\n",
    "auto_svc.fit(X_train,y_train)\n",
    "print('OOB score=', auto_svc.score(X_test, y_test))\n",
    "y_pred3 = auto_svc.predict(X_test)\n",
    "MSE2 = mean_squared_error(y_test, y_pred3)\n",
    "print('MSE=', MSE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandBestParams1= {'C': 0.3377990724342859, 'gamma': 'scale', 'shrinking': False}\n",
      "RandBestScore1= 0.11734693877551021\n"
     ]
    }
   ],
   "source": [
    "### CVM + Tunning parameters\n",
    "\n",
    "param_dist4 = {'C': sp_uniform(loc=0.2, scale=4.0),\n",
    "              'gamma': ['scale', 'auto'],\n",
    "              'shrinking': [True, False]}\n",
    "\n",
    "\n",
    "hit_tree4_loop = svm.SVC()\n",
    "\n",
    "\n",
    "# Run randomized hyperparameter search\n",
    "random_search2 = \\\n",
    "    RandomizedSearchCV(hit_tree4_loop, param_distributions=param_dist4,\n",
    "                       n_iter=100, n_jobs=-1, cv=4, random_state=25,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "\n",
    "random_search2.fit(XplusConst_auto, y_auto)\n",
    "#print('RandBestEstimator1=', random_search2.best_estimator_)\n",
    "print('RandBestParams1=', random_search2.best_params_)\n",
    "print('RandBestScore1=', -random_search2.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic model does better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Neural Network Horse race\n",
    "\n",
    "Test the predictive accuracy of three models on classifying wines into one of three possible cultivars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('strongdrink.txt', sep=\",\")\n",
    "### Divide the data between X and Y\n",
    "X = wine[[\"alco\", \"malic\", \"tot_phen\", \"color_int\"]].values\n",
    "y = wine[\"cultivar\"].values\n",
    "#wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cultivar\n",
       "1    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "2    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "3    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: color_int, dtype: object"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wine.set_index('alco', inplace=True)\n",
    "wine.groupby('cultivar')['color_int'].plot(marker='.', \n",
    "                                           linestyle='',\n",
    "                                           legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the factors that make miles per gallon high or low\n",
    "y = wine[\"cultivar\"].values\n",
    "X = wine[[\"alco\", \"malic\", \"tot_phen\", \n",
    "          \"color_int\"]].values\n",
    "\n",
    "# Add a constant\n",
    "num_obs = X.shape[0]\n",
    "const_vec = np.ones(num_obs).reshape((num_obs, 1))\n",
    "XplusConst = np.hstack((const_vec, X))\n",
    "\n",
    "\n",
    "### Divide the data between test and train\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(XplusConst, y, test_size = 0.25,\n",
    "       random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated coefficiens and intercepts for j=1  -20.292795890400747 [-2.55911111e-07  1.25230318e+00 -2.04969246e-01  1.45813522e+00\n",
      "  4.09179614e-01]\n",
      "Estimated coefficiens and intercepts for j=2  20.92046514680566 [ 2.74572997e-06 -1.05122227e+00 -4.40058064e-01  5.55367840e-01\n",
      " -1.71913227e+00]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      1.00      0.83        12\n",
      "           2       1.00      0.75      0.86        24\n",
      "           3       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.84        44\n",
      "   macro avg       0.83      0.88      0.84        44\n",
      "weighted avg       0.88      0.84      0.84        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Fit a multinomial logistic model \n",
    "\n",
    "### Run the MLHE of out multiple logistic regression classifier model.\n",
    "LogReg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "#LogReg = LogisticRegression()\n",
    "\n",
    "LogReg.fit(X_train, y_train)\n",
    "y_pred = LogReg.predict(X_test)\n",
    "\n",
    "print(\"Estimated coefficiens and intercepts for j=1 \", LogReg.intercept_[0], LogReg.coef_[0,:])\n",
    "print(\"Estimated coefficiens and intercepts for j=2 \", LogReg.intercept_[1], LogReg.coef_[1,:])\n",
    "\n",
    "### Report the error rates (1 - precision)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandBestParams1= {'C': 2.424917300687881, 'gamma': 'auto', 'shrinking': False}\n",
      "RandBestScore1= 0.1534090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "### Optimally tune the hyperparameters penalty and C in the logistic regression\n",
    "\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "param_dist1 = {'penalty': ['11', '12'],\n",
    "             'C': sp_uniform(0.1, 10.0)}\n",
    "\n",
    "hit_wine_SVC = svm.SVC()\n",
    "\n",
    "# Run randomized hyperparameter search\n",
    "random_search2 = \\\n",
    "    RandomizedSearchCV(hit_wine_SVC, param_distributions=param_dist4,\n",
    "                       n_iter=200, n_jobs=-1, cv=4, random_state=25,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "\n",
    "random_search2.fit(XplusConst, y)\n",
    "#print('RandBestEstimator1=', random_search2.best_estimator_)\n",
    "print('RandBestParams1=', random_search2.best_params_)\n",
    "print('RandBestScore1=', -random_search2.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandBestEstimator1= RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=4,\n",
      "                      max_features=2, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=2, min_samples_split=19,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "RandBestParams1= {'max_depth': 4, 'max_features': 2, 'min_samples_leaf': 2, 'min_samples_split': 19, 'n_estimators': 10}\n",
      "RandBestScore1= 0.5628096729280687\n"
     ]
    }
   ],
   "source": [
    "### Fit a random forest model of cultivar on the same 4 variables as above.\n",
    "### Optimally tune the parameters\n",
    "from scipy.stats import randint as sp_randint\n",
    "param_dist2 = {'n_estimators': [10, 200],\n",
    "               \"max_depth\": [2, 4],\n",
    "               \"min_samples_split\": sp_randint(2, 20),\n",
    "               \"min_samples_leaf\": sp_randint(2, 20),\n",
    "               \"max_features\": sp_randint(1, 4)}\n",
    "\n",
    "\n",
    "hit_wine_RF = RandomForestRegressor()\n",
    "\n",
    "\n",
    "# Run randomized hyperparameter search\n",
    "random_search2 = \\\n",
    "    RandomizedSearchCV(hit_wine_RF, param_distributions=param_dist2,\n",
    "                       n_iter=200, n_jobs=-1, cv=4, random_state=25,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "\n",
    "random_search2.fit(XplusConst, y)\n",
    "print('RandBestEstimator1=', random_search2.best_estimator_)\n",
    "print('RandBestParams1=', random_search2.best_params_)\n",
    "print('RandBestScore1=', -random_search2.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandBestParams1= {'C': 8.469281639209688, 'gamma': 'scale', 'shrinking': True}\n",
      "RandBestScore1= 0.1534090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "### Fit a support vector machines classifier model of cultivar with a Gaussian radial basis\n",
    "# function kernel\n",
    "\n",
    "param_dist4 = {'C': sp_uniform(loc=0.1, scale=10.0),\n",
    "              'gamma': ['scale', 'auto'],\n",
    "              'shrinking': [True, False]}\n",
    "\n",
    "\n",
    "hit_wine_SVC = svm.SVC()\n",
    "\n",
    "\n",
    "# Run randomized hyperparameter search\n",
    "random_search2 = \\\n",
    "    RandomizedSearchCV(hit_wine_SVC, param_distributions=param_dist4,\n",
    "                       n_iter=200, n_jobs=-1, cv=4, random_state=25,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "\n",
    "random_search2.fit(XplusConst, y)\n",
    "#print('RandBestEstimator1=', random_search2.best_estimator_)\n",
    "print('RandBestParams1=', random_search2.best_params_)\n",
    "print('RandBestScore1=', -random_search2.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandBestParams1= {'activation': 'logistic', 'alpha': 3.0301188113468682, 'hidden_layer_sizes': 81}\n",
      "RandBestScore1= 0.09183673469387756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "### Fil multiple hidden layer neural network (multiple layer perceptron)\n",
    "# model of cultiver\n",
    "\n",
    "import sklearn.neural_network as neural_network\n",
    "\n",
    "param_dist4 = {'hidden_layer_sizes': sp_randint(1, 100),\n",
    "               'activation': ['logistic', 'relu'],\n",
    "               'alpha': sp_uniform(0.1, 10.0)}\n",
    "\n",
    "\n",
    "hit_wine_MLPC = neural_network.MLPClassifier()\n",
    "\n",
    "\n",
    "# Run randomized hyperparameter search\n",
    "random_search2 = \\\n",
    "    RandomizedSearchCV(hit_wine_MLPC, param_distributions=param_dist4,\n",
    "                       n_iter=200, n_jobs=-1, cv=4, random_state=25,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "\n",
    "random_search2.fit(XplusConst_auto, y_auto)\n",
    "#print('RandBestEstimator1=', random_search2.best_estimator_)\n",
    "print('RandBestParams1=', random_search2.best_params_)\n",
    "print('RandBestScore1=', -random_search2.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best approach is the neural net.\n",
    "The logistic regression then does better than the machine classifier and SVC. Random forest doesn't do very well.\n",
    "\n",
    "\n",
    "The neural net is the most flexible and sophisticated approach. \n",
    "The RF doesn't do very well because there are not many parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
